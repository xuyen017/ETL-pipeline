{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%help",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 5.0\n%worker_type G.1X\n%number_of_workers 5\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "#### Example: Create a DynamicFrame from a table in the AWS Glue Data Catalog and display its schema\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "dyf = glueContext.create_dynamic_frame.from_catalog(database='etl-project-for-database', table_name='raw_data2')\ndyf.printSchema()",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "#### Convert the DynamicFrame to a Spark DataFrame and display a sample of the data\n",
			"metadata": {
				"editable": true,
				"tags": [],
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "df = dyf.toDF()\ndf.show()",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "#### Drop columns that we don't need it\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "df = df[\"id\",\"year_birth\",\"education\",\"marital_status\",\"income\",\"dt_customer\"]\ndf.show(10)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "#### Check NaN values for each column\n",
			"metadata": {
				"editable": true,
				"tags": [],
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import *\n\ndf.select([count(when(col(c).isNull(),c)).alias(c) for c in df.columns]).show()",
			"metadata": {
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "There are 24 NaN values in \"income\" column. Let's fill NaN values with mean.",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# Calculate the mean value of the column\nmean_value = df.select(mean(col('income'))).collect()[0][0]\n\n# Fill missing values with the mean value\ndf = df.fillna(mean_value, subset=['income'])\n\n# Check\ndf.select([count(when(col(c).isNull(),c)).alias(c) for c in df.columns]).show()",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "#### Write the data to our S3 Bucket named \"transformed_data\" as csv.",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "df.write \\\n    .format(\"csv\") \\\n    .mode(\"append\") \\\n    .option(\"header\", \"true\") \\\n    .save(\"s3://etl-bucket-s3/etl-bucket-s3-database/transformed_data/\")",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "#### Write the data to our S3 Bucket named \"transformed_data\" as json.",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "df.write \\\n    .format(\"json\") \\\n    .mode(\"append\") \\\n    .save(\"s3://etl-project-for-medium/etl-project-for-medium-database/transformed_data/\")",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}